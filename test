package com.schwab.cdt.spos.source.service;

import com.schwab.cdt.spos.source.config.CsvToAvroConfig.Conversion;
import com.schwab.cdt.spos.source.config.CsvToAvroConfig.Conversion.FieldMapping;
import com.schwab.cdt.spos.source.spos.SposStreamingClient;
import com.schwab.cdt.spos.source.config.SposConfig;
import org.apache.avro.Conversions;
import org.apache.avro.LogicalTypes;
import org.apache.avro.Schema;
import org.apache.avro.data.TimeConversions;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.*;
import org.apache.avro.io.DatumWriter;
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.core.sync.RequestBody;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.*;
import software.amazon.awssdk.services.s3.model.*;
import org.springframework.stereotype.Component;

import java.io.*;
import java.math.BigDecimal;
import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.time.format.DateTimeFormatterBuilder;
import java.util.*;
import java.util.stream.Collectors;

@Component
public class CsvToAvroConverter {

    private final AvroSchemaGenerator avroSchemaGenerator;
    private final SposStreamingClient sposStreamingClient;
    private final S3Client s3Client;
    private final SposConfig sposConfig;

    public CsvToAvroConverter(AvroSchemaGenerator avroSchemaGenerator,
                              SposStreamingClient sposStreamingClient,
                              SposConfig sposConfig) {
        this.avroSchemaGenerator = avroSchemaGenerator;
        this.sposStreamingClient = sposStreamingClient;
        this.sposConfig = sposConfig;
        this.s3Client = createSposS3Client();
    }

    /**
     * Converts CSV data to Avro format and uploads it to SPOS using multipart upload.
     *
     * @param conversion      The conversion configuration.
     * @param sourceDirectory The source directory containing CSV files.
     * @throws IOException If an I/O error occurs.
     */
    public void convertCsvToAvro(Conversion conversion, String sourceDirectory) throws IOException {
        // Generate Avro schema based on the conversion configuration
        Schema schema = avroSchemaGenerator.generateSchema(conversion);

        // Since the schema is embedded in the Avro file, we can remove the writeSchemaToSpos method
        // writeSchemaToSpos(schema, conversion.getAvroSchemaOutput());

        String delimiter = getDelimiter(conversion.getDelimiter());
        boolean skipFirstLine = conversion.getSkipPolicy().isSkipFirstLine();

        // Map field names to their mappings for easy access
        Map<String, FieldMapping> fieldMappings = conversion.getFieldMappingList().stream()
                .collect(Collectors.toMap(FieldMapping::getName, fm -> fm));

        // Prepare Avro generic data with logical type conversions
        GenericData genericData = new GenericData();
        genericData.addLogicalTypeConversion(new Conversions.DecimalConversion());
        genericData.addLogicalTypeConversion(new TimeConversions.DateConversion());

        // Fetch the latest CSV file from SPOS
        String csvFileUrl = fetchLatestCsvFileFromSpos(sourceDirectory);
        InputStream csvInputStream = sposStreamingClient.streamObjectStore(csvFileUrl);
        if (csvInputStream == null) {
            throw new IOException("Could not open CSV file");
        }

        // Create a temporary file to store the Avro data
        File tempAvroFile = File.createTempFile("tempAvro", ".avro");

        try (DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<>(new GenericDatumWriter<>(schema, genericData));
             BufferedReader br = new BufferedReader(new InputStreamReader(csvInputStream))) {

            dataFileWriter.create(schema, tempAvroFile);

            String line;
            List<String> headers = null;

            // Skip the first line if required and extract headers
            if (skipFirstLine) {
                line = br.readLine();
                if (line != null) {
                    headers = Arrays.asList(line.split(delimiter, -1));
                }
            }

            if (headers == null) {
                throw new IllegalArgumentException("CSV file does not contain headers, but headers are required.");
            }

            // Map CSV headers to their indices
            Map<String, Integer> fieldNameToIndexMap = new HashMap<>();
            for (String fieldName : fieldMappings.keySet()) {
                int index = headers.indexOf(fieldName);
                if (index == -1) {
                    throw new IllegalArgumentException("Field '" + fieldName + "' not found in CSV headers.");
                }
                fieldNameToIndexMap.put(fieldName, index);
            }

            // Read CSV lines and write to Avro file
            while ((line = br.readLine()) != null) {
                String[] values = line.split(delimiter, -1);

                GenericRecord record = new GenericData.Record(schema);

                for (Map.Entry<String, Integer> entry : fieldNameToIndexMap.entrySet()) {
                    String fieldName = entry.getKey();
                    int index = entry.getValue();
                    String csvValue = index < values.length ? values[index] : null;

                    FieldMapping fieldMapping = fieldMappings.get(fieldName);
                    Schema.Field field = schema.getField(fieldName);
                    Schema fieldSchema = field.schema();
                    Object convertedValue = convertValue(csvValue, fieldSchema, fieldMapping);
                    record.put(fieldName, convertedValue);
                }
                dataFileWriter.append(record);
            }
            dataFileWriter.flush();
        } catch (Exception e) {
            throw new RuntimeException("Error during CSV to Avro conversion", e);
        }

        // Upload the Avro file to SPOS using multipart upload
        String keyName = conversion.getTargetAvroFile();
        uploadAvroFileMultipart(keyName, tempAvroFile);

        // Delete the temporary Avro file
        boolean deleted = tempAvroFile.delete();
        if (!deleted) {
            System.err.println("Warning: Temporary Avro file could not be deleted: " + tempAvroFile.getAbsolutePath());
        }
    }

    /**
     * Uploads an Avro file to SPOS using multipart upload.
     *
     * @param keyName  The object key in SPOS.
     * @param avroFile The Avro file to upload.
     */
    private void uploadAvroFileMultipart(String keyName, File avroFile) {
        // Step 1: Initiate multipart upload
        CreateMultipartUploadRequest createMultipartUploadRequest = CreateMultipartUploadRequest.builder()
                .bucket(sposConfig.bucket())
                .key(keyName)
                .build();

        CreateMultipartUploadResponse createMultipartUploadResponse = s3Client.createMultipartUpload(createMultipartUploadRequest);
        String uploadId = createMultipartUploadResponse.uploadId();

        List<CompletedPart> completedParts = new ArrayList<>();
        int partNumber = 1;
        long partSize = 5 * 1024 * 1024; // 5 MB
        long contentLength = avroFile.length();

        try (InputStream inputStream = new FileInputStream(avroFile)) {
            byte[] buffer = new byte[(int) partSize];
            int bytesRead;
            long filePosition = 0;

            // Read the file and upload in parts
            while ((bytesRead = inputStream.read(buffer)) != -1) {
                UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                        .bucket(sposConfig.bucket())
                        .key(keyName)
                        .uploadId(uploadId)
                        .partNumber(partNumber)
                        .contentLength((long) bytesRead)
                        .build();

                RequestBody requestBody = RequestBody.fromBytes(Arrays.copyOf(buffer, bytesRead));

                UploadPartResponse uploadPartResponse = s3Client.uploadPart(uploadPartRequest, requestBody);

                completedParts.add(CompletedPart.builder()
                        .partNumber(partNumber)
                        .eTag(uploadPartResponse.eTag())
                        .build());

                filePosition += bytesRead;
                partNumber++;
            }

            // Step 3: Complete multipart upload
            CompletedMultipartUpload completedMultipartUpload = CompletedMultipartUpload.builder()
                    .parts(completedParts)
                    .build();

            CompleteMultipartUploadRequest completeMultipartUploadRequest = CompleteMultipartUploadRequest.builder()
                    .bucket(sposConfig.bucket())
                    .key(keyName)
                    .uploadId(uploadId)
                    .multipartUpload(completedMultipartUpload)
                    .build();

            s3Client.completeMultipartUpload(completeMultipartUploadRequest);

            System.out.println("Multipart upload completed successfully.");

        } catch (Exception e) {
            e.printStackTrace();

            // Abort multipart upload in case of failure
            AbortMultipartUploadRequest abortMultipartUploadRequest = AbortMultipartUploadRequest.builder()
                    .bucket(sposConfig.bucket())
                    .key(keyName)
                    .uploadId(uploadId)
                    .build();
            s3Client.abortMultipartUpload(abortMultipartUploadRequest);
            System.out.println("Multipart upload aborted due to failure.");

            throw new RuntimeException("Error during multipart upload", e);
        }
    }

    /**
     * Converts a CSV value to the appropriate Avro type.
     *
     * @param value           The CSV value as a string.
     * @param fieldSchema     The Avro schema of the field.
     * @param fieldDefinition The field mapping definition.
     * @return The converted value.
     * @throws Exception If conversion fails.
     */
    private Object convertValue(String value, Schema fieldSchema, FieldMapping fieldDefinition) throws Exception {
        if (value == null || value.isEmpty()) {
            return null;
        }
        Schema actualSchema = getNonNullSchema(fieldSchema);
        switch (actualSchema.getType()) {
            case INT:
                if (actualSchema.getLogicalType() != null && "date".equals(actualSchema.getLogicalType().getName())) {
                    DateTimeFormatter format = new DateTimeFormatterBuilder()
                            .parseCaseInsensitive()
                            .appendPattern(fieldDefinition.getFormat())
                            .toFormatter();
                    LocalDate date = LocalDate.parse(value, format);
                    return (int) date.toEpochDay();
                }
                return Integer.parseInt(value);
            case LONG:
                return Long.parseLong(value);
            case STRING:
                return value;
            case BOOLEAN:
                return Boolean.parseBoolean(value);
            case BYTES:
                if (actualSchema.getLogicalType() != null && "decimal".equals(actualSchema.getLogicalType().getName())) {
                    BigDecimal decimalValue = new BigDecimal(value);
                    LogicalTypes.Decimal decimalType = (LogicalTypes.Decimal) actualSchema.getLogicalType();
                    return new Conversions.DecimalConversion().toBytes(
                            decimalValue,
                            actualSchema,
                            decimalType
                    );
                }
                return ByteBuffer.wrap(value.getBytes());
            default:
                throw new UnsupportedOperationException("Unsupported field type: " + actualSchema.getType());
        }
    }

    /**
     * Retrieves the non-null schema from a union schema.
     *
     * @param schema The schema to process.
     * @return The non-null schema.
     */
    private Schema getNonNullSchema(Schema schema) {
        if (schema.getType() == Schema.Type.UNION) {
            for (Schema type : schema.getTypes()) {
                if (type.getType() != Schema.Type.NULL) {
                    return type;
                }
            }
            throw new IllegalArgumentException("No non-null type found in union schema: " + schema);
        } else {
            return schema;
        }
    }

    /**
     * Fetches the latest CSV file from SPOS.
     *
     * @param sourceDirectory The source directory in SPOS.
     * @return The URL of the latest CSV file.
     * @throws IOException If an error occurs while fetching the file.
     */
    private String fetchLatestCsvFileFromSpos(String sourceDirectory) throws IOException {
        List<String> objectKeys = sposStreamingClient.getObjects(sourceDirectory, 1);
        if (objectKeys.isEmpty()) {
            throw new IOException("No objects found in source directory");
        }
        return sposStreamingClient.getObjectUrl(objectKeys.get(0));
    }

    /**
     * Converts a delimiter string to the actual delimiter character.
     *
     * @param delimiter The delimiter string.
     * @return The actual delimiter character.
     */
    private String getDelimiter(String delimiter) {
        switch (delimiter.toUpperCase()) {
            case "COMMA":
                return ",";
            case "PIPE":
                return "\\|";
            default:
                return delimiter;
        }
    }

    /**
     * Creates an S3Client configured for SPOS.
     *
     * @return The configured S3Client.
     */
    private S3Client createSposS3Client() {
        AwsBasicCredentials awsCreds = AwsBasicCredentials.create(sposConfig.awsAccessKey(), sposConfig.awsSecretKey());

        return S3Client.builder()
                .endpointOverride(URI.create(sposConfig.url()))
                .credentialsProvider(StaticCredentialsProvider.create(awsCreds))
                .region(Region.of(sposConfig.region()))
                .serviceConfiguration(S3Configuration.builder()
                        .pathStyleAccessEnabled(true) // Important for SPOS if virtual-hosted-style is not supported
                        .build())
                .build();
    }
}